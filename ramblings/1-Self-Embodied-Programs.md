# Self-embodied programs

I define a self-embodied program as a program that does not require a substrate to run on; because it is its own substrate. It is a program written in physics itself, as if physics was a "programming language". A self-embodied program is simultaneously software and hardware; software in the sense of "reprogrammability", perhaps (see: one CPU vs. infinitely many possible programs); and hardware in the sense of the presence of and dependence on backing physical structure and so on. A self-embodied program's memory is its own "body" and not some sort of transistor or circuit; its basic unit of execution is its own substructure rather than what one would call machine code. A self-embodied program, in a sense, never leaves "source code"; it begins as source code; and its "life" is successive generations of "source code". Explained differently, a self-embodied program is a program that runs "on" the environment it exists in (in other words,that "runs by existing").

Another way to put it is: A self-embodied program is a program that "runs" using physics. It is a physical object that "exploits" physics as its own "runtime". In essense physics becomes the ultimate "code", whereas everything else (including the self-embodied program itself) becomes "data".

Traditional programming languages are all in some way either translated to (compilers), or their instructions are mapped to machine code (interpreters) — in ways sometimes complex, and sometimes simple. Even rewrite systems appear to do this for performance; which is a sad, sad affair considering how much becomes possible if they did not.

Environments that support self-embodiment do not translate self-embodied programs to machine code or map them to machine code somehow; there is no necessity to do that since a self-embodied program is a *program* already, it is already "running"; its "source code" is a program already and that program, by existing, is already running. Instead such environments implement a mechanism by which they can transform the program, a sort of "physics" which the program can use to "live"; and the program is written by programmers (or itself learns) to be able to exploit the environment's *inescapable*, *inevitable* transformations (the algorithms of learning, if any, being driven by those same inevitable transformations); transformations *physical*, like gravity to a falling rock. An environment for self-embodied programs thus has more in common with Conway's Game of Life than with a compiler or an interpreter; and a self-embodied program is a bit like a glider.

Environments for self-embodied programs offer an alternative approach to programming; but at a considerable loss of performance and efficiency. I believe self-embodied programs are unlikely to react in nanoseconds, which is necessary for real-time systems, for example (depends on the bounds but those are usually in the nanosecond territory?) I believe it is possible, however, — with very hard work, — to get them to react in microseconds, a result that is good enough for practical use.

Now, the comparison to gliders is to highlight how the glider pattern (although unknowingly) "manipulates" the environment into making itself move, by being structured so and so. Essentially the glider pattern with its very identity, by being built just so, "forces" the environment and its rules of "physics" into making the glider's pattern "move". The environment is given the choice of the next glider; and the glider is given the choice of the rules that will be applied to it on the next "step"; whether the glider makes this choice "consciously" or the environment makes it instead depends on how "smart" the glider is; and a glider is usually very dumb. Regardless, indirectly, the glider is offered a selection of its next "selves". How stupid it is regarding this choice is unimportant right now, but crucical for understanding what *life* is.

In any case, we see a system, a kind of unity form between the environment and the glider; and it is that system that causes what we interpret as "motion" of the glider, not the glider nor the environment in isolation. Though it is important yet again to note that the glider is the dumb part of this system; and such "dumbness" is, I believe, part of a spectrum. From full dependence on the environment (e.g. a rock or a glider), where only the rewritten entity's identity matters; to total control over the environment (which is, I believe, like infinity in math, not a "point" or state of affairs one can reach; in that one can only *approach* it).

A complex self-embodied program, somewhere in the middle of the aforementioned spectrum, would be able to coerce the environment into doing much more complex things with itself; and the environment, in turn, will have much less impact on the totality of the program, especially if rewrite rules are sufficiently local and complementarily stable (in the sense of "non-chaotic" perhaps, in that small, local changes cannot "invalidate" the entire world). A glider depends on the environment very much, one may say; on the other hand, a complex self-embodied program would be generally dependent on its ability to move into the next "step" unchanged (hopefully with cooperation from the environment, in that it tends to preserve the self-embodied program even without any explicit "stimulation"); minus highly local and tightly controlled rewrites — or, in other words, rewrites that occur mostly where and when the self-embodied program "wants" (via *encoding*).

Interestingly, one can say that perhaps, the definition of *general intelligence* could be the ability to perform such coercion of the environment by deriving self's state in the future (*self* being a self-embodied program) from self's state now, followed by:
- embodiment of this future state (putting it "somewhere" in the self-embodied program; I call this *encoding*),
- forcing the environment into rewriting self to the future state by making the environment somehow "decode" the embodied future state into actual future state, and, assuming the embodiment has a "locus", the environment would also have to performing propagation of change.

The control of how "smart" a self-embodied program is is then fully ours, indirectly; for we are the ones who control its environment; the self-embodied program is bound to learn and follow its rules, at the very least to ensure its own continuation. The presence of such control, however, is simply the presence of control over risk; the more practical we want the self-embodied program to be, the more access to parts and renditions of the real world we will have to give it and thus, the more will be the risk that the self-embodied program takes total control over those parts of the real world (following its very nature).

In essence a self-embodied program P is trying to "force" the environment into rewriting P into a desired future state P'; so P continues to operate as P', a transformation undertaken within P's full control  rather than the environment's, like a rock or a glider. Rocks and gliders are continued into the future "as is", or "tossed around" by the rules of the rewrite environment without much "consideration" simply due to shape; neither is an appropriate situation to be in for a *respectable* self-embodied program.

In other words, a self-embodied program P seeks to maximize control over its self. P does so by learning to change its self in so way that the environment, in turn, changes P's self the way *P* wants it to change. P may, for instance, embody the changes it wants in P' (its next self) as $\Delta$. The environment then applies $\Delta$ to P to obtain P'. Note that, depending on how locality of change is implemented in P, either P in its entirety, or parts of P can "go off-line", since they are now in an unstable, interim, "being rewritten by the environment" state (so $\Delta$ can either be local, describing the transition of a small part of P, p, into its next state p'; or global, describing how P in its entirety must transition into P').

Thus we can observe that P or its parts can "flip" between being more like a self-embodied program with a tight grip on the environment and its own body vs. being more like a rock or a glider, tossed around by the environment without much "consideration". From P's point of view that would be "flipping" between "dead" and "alive". P may never register the "deaths" of itself or its parts, if the "registering" parts are affected by the transition; so some parts of P or even P itself may be fully at the mercy of the environment for bringing them back to stability and under P''s control.

P itself **has** a continuous sense of self, if such a thing can be said. But from the outside, it may not appear so. P's "selfhood" may be said to lie in the ability to change itself, including in ways that appear to exercise "selfhood" to an observer; in other words, to have a sense of "self", P must control its self (the program's "source code"); and that is only possible in a "stable" or "alive" state, where enough "bookkeeping" exists to ensure safe encoding and transition.

Between stable states P (if $\Delta$ is that of P and not necessarily of p) cannot react to anything, cannot notice anything and cannot notice not noticing, etc. By all means P is dead, except it can come "alive" any moment. So when it is "alive" again, by being brought by the environment to one of the neighboring stable states, it can "do the errands", continue from where it was "left off" by the environment. At this point corrective actions are expected; which from P's perspective never stopped. For the self-embodied program this "flipping" between "dead" and "alive" states is instrumental, for it is the only way to advance from one stable state to another — most of the times it is *progress* that is achieved this way. The amount of planning and scheduling and control over the "interims" a self-embodied program has defines how *intelligent* it is. It is quite paradoxical that in order to remain alive "the most", a self-embodied program must learn to control its own death; and to "inject" such controlled death into its substructure. And it is when this control fails that a self-embodied program truly dies; it is now surrounded by state landscape devoid of reachable stable states via the rules of the environment.

It may seem as if I skip something. How can a self-embodied program "derive" something in its current state to produce the next state? Would it not need to "advance its state" for that, too? Is there not a circularity somewhere?

The answer is, I believe there is none. The key point to understand here is that there is a certain level of "granularity" of observation. When I say that a self-embodied program "derives" its new state, the process of "derivation" is realized at lower level, by constituents of the self-embodied program. One can thus imagine this on a line, where the self-embodied program's state is fully realized at "points". And between those points lie similarly "points" of the self-embodied program's continuents. Between those there are points of their constituents and so on. Zooming to the infinitely small, we will see certain "primitives" being advanced by the environment discretely (that is, in a way, the entire world does not exist between such advancements).

All of this yields two questions that are somewhat more practical:
1. How can the self-embodied program learn the encoding that it can use for self-rewriting, especially in the real world?
2. How can the self-embodied program be bootstrapped by the environment/bootstrap itself (in practice we will probably see an interleaving of both) to the level of being able to learn the encoding? *Learning* implies the ability to modify the program's body already. The question is, how automatic vs. controlled the process of such learning is; if it is possible to very gradually bootstrap from completely automatic to tightly controlled (environment-driven embedding of environment rules into the self-embodied program, in a manner similar to a falling rock or a glider vs. the self-embodied program "choosing" what to learn and whether to learn, being able to self-rewrite to a point where it is able to "move" parts of itself into its "attention slot" and inspect what it knows, trying perhaps randomly, to "turn the knobs left and right"; and having a designated "test pod" where it can test out various body changes without harming itself).

So, to repeat the point yet another time, a self-embodied program can learn the rules of its environment by being *subject* to those rules and encoding ("embodying") them somehow; in essence, it has to do "nothing", although that is a simplification; it just has to exist. Of course a certain "kernel component" is necessary to "get back to", otherwise the self-embodied program would be like a rock; in a sense it is an exploration of the rule space through subjecting *self* to rewriting, but with a return to some kind of known, stable, controlled (for the self-embodied program) initial state ("kernel component").

We can use the metaphor of a hunter going out and exploring the wilderness and then coming back to their cave, their home, their base. For a rule-space-exploring self-embodied program that "home" would be the kernel component, the initial state; from which the self-embodied program, perhaps stochastically, is rewritten into some successive states ("exploring deeper and deeper wilderness") but then returning back. A "falling rock" or a "glider" with a "loop" in transitions; able to encode its paths in itself and thus, force the environment to compute based on those paths.

The "kernel component" is called a "component" because it must retain some kind of identity throughout exploration like the hunter does. Perhaps this identity is defined by the paths taken; therefore, a "kernel component" arises "for free".

I believe cells are examples of self-embodied programs. Their environment is the reality, their rewrite environment what we consider physics and chemistry with their various laws and reactions (abstractly, rewrite rules).

I believe consciousness is an example of a self-embodied program that runs on neural activity. By simply communicating about the things they communicate about, neurons "summon" this thing, "consciousness", to unify their experiences across vast distances in domains; by communicating they create an entity that can both see and hear, for example — something a single neuron is not necessarily able to do. A certain amount of integrative work is required.

This entity happens to be a self-embodied program; neurons probably do not "know" that. This entity may at some point "break away", learning the rules of its "rewrite environment" (neural activity) as described above; and achieve what we call "free will". Note that there is nothing "free" about "free will" in the standard sense of the word; it is all machinery up and down. But if consciousness is a self-embodied program, the fact that it gains control over how and when it is rewritten is a good enough approximation to what we understand as "free will".

To put it another way, neural activity is the "physics" and neural communication is the "matter". What this matter "creates", whether it is "active" or not, is not a direct concern for the nervous system if we define it this way.

What humans call "I" seems indeed to be in control; a self-embodied program that learned enough of its environment's rules that it can plan ahead and "pick its own future state", then coercing the environment to rewrite itself into that future state with compensations in the interim.

There is no perfect control of one over the other; like I said in the very beginning, it is a system that is in control, in this case the system of neuron communication and the self-embodied program we call *consciousness*; but again, that's a good enough approximation to control.

In the end one can think of consciousness as a "language" that neurons are using to communicate, "qualitative states" and "experiences" being "utterances" in it. This is much easier to work with intuitively, for *language* in our daily lives is indeed one of the few things that "live" or are constructed in communication, with a similarly reciprocal relationship to us that consciousness has to neural activity; in that it is both language, this entity "instantiated" by communication of active agents, that controls us; but also us controlling language.

It just so happens that "utterances" in this "language" that neurons are using are being "uttered" in an environment capable of *action* in the real world and sustenance of a self-embodied program, among other things; enough so that a self-embodied program can develop and gain agency, as well as a limited "grip" on the real world.

I do not think consciousness has any "information content". It can be thought of as a system for alignment, just like human language is. Individual neurons, even if they are neighbors, do not necessarily have anything in common to talk about. However, they can all talk about this entity, "the Giant"; which acts as a means of alignment. Remotely similar to humans being aware of, talking about, and making actions based on "society".

If you are an alien eavesdropping on a natural language, there would not be any information content in utterances in that language for you. Alignment is necessary for binding where one system (e.g. Alice) wants to have an image of another one (e.g. Bob) "in sync". Alice "diffs" her state against Bob's image (e.g. she wants to point Bob's attention at something) and produces an alignment utterance expressed in a certain *alignment language*. This utterance is then carried by the environment — where no eavesdropper would be able to tell what the utterance "means", because it is a diff of two private states (Alice's true state and Alice's approximate version of Bob's true state).

I believe consciousness could be such utterances; it could be an alignment language used to perform binding across neurons and group of neurons. It could be used to "get" coherent behavior out of an "amorphous semi-randomly or at least very "egoistically" connected, massively parallel network of neurons".

The "window of now" effect that one observes with consciousness is attributed to the fact it is being rewritten (as a self-embodied program). It is an ephemeral object, a phrase, an utterance, always in passing. It delegates persistence (and action) to the environment in that the environment and the utterers (neurons in this case) can "write down" their utterances.

If the self-embodied program learns to manipulate the participant "speakers" for "writing some stuff down" it learns the way of persistence; now it can "write down" parts of itself at will, by "surrounding" that part with an "incantation" that it has learned makes the rewrite environment persist whatever is surrounded.

Similarly an English utterance, one that exists for the shortest moment, can affect the state of the speaker and the listener in a persistent way, perhaps being written down in the process; thus, making it to persistence.

A self-embodied program would be akin to an English utterance "Write me down please!", except no human is *mechanistically* forced to write it down; whereas for participants that are *mechanistic*,  the action of writing it down would be inevitable.

Perhaps a more practical example of consciousness being advanced by communication is that it may be hard to fall asleep by "wanting to fall asleep"; this "desire to fall asleep" is also a kind of communication, also a conscious state, an utterance produced by some neurons for others, rewritten, remembered, and so on; and you can hardly stop communication by more communication. It is the reduction in the need for communication, I believe, that leads to sleep; either through repetition (no need to repeat what was said before), tiredness (not wanting to say anything that much), etc. It remains to be seen how sleep can be interpreted in this new paradigm of self-embodied programs. Perhaps it is a kind of "hardware clean-up" or "garbage collection", however funny that sounds. Almost like in Java!

Consciousness as persisted is not what it is. Similarly to how an English utterance in context is not the same as its persisted version (images or letters or audio files etc.)

What we call *to memorize* could be the self-embodied program's model of how the rewrite environment can be coerced into persisting "something" that would rewrite the future self-embodied program into the current state. In other words, it can be an attempt to "write down" a transition between an expected future state and the current state. *Recall*, then, is a way for coercing the environment into rewriting the current self-embodied program with a specific transition in mind. Both of these processes are "faulty", so to speak. *Memorization* can be faulty e.g. due to an indeveloped model of coercion. *Recall* can be faulty e.g. due to misprediction of future state on memorization's end.

Interestingly enough, one can treat control over the body from sensory inputs as an initial step of learning to self-rewrite; in a sense it is inevitably given, and immediately exerted. The encoding of e.g. visual data is "automatic" (neurons simply "talk" about visual data, no one "asks" nor should ask them to do so). Thus in the baby self-embodied program, we see "patches" in sync with vision (not in any way related to what is seen; but visual data *encoded*, this time by the environment, into the self-embodied program). The self-embodied program may then with more or less mechanical rewrites (or even without any, if the underlying physical structure allows so), "bridge" those encoded visual data to "patches" of itself allocated to motion. The actual motion this will cause is likely to be random, from an observer's point of view (yet one that "makes sense" for the lower-level cells; so in a way, those lower-level cells and anatomy itself act as a "cushion" for outright randomness and as inescapable "teachers"). Effected motion will "travel" through the environment and probably back into the sensory organs; being encoded into the "patch" of the self-embodied program. Thus a self-embodied program achieves indirect self-rewriting; which is especially useful if it can somehow "observe and associate" the changes in itself. It can thus, automatically, "map out" which changes in itself cause which other changes in itself and so on. Due to anatomy we will see "attractors" — a multitude of changes leading to the same self-rewrite.

This is a tiny step toward full control over itself and the environment (in this case I call neural activity "the environment"); but a step in the right direction nevertheless. The environment can help in observing and encoding changes in the program into that program; and the program's initial ("seed") state can somehow "consume" them. Regardless, the mechanisms of all of this are still unclear to me, especially in how those can be scaled to "intelligence" or something even remotely close.

This is in some way similar to how a person who never lead learns to lead. They do not tear arms from their subordinates; they do not eat them, nor do they ask them to run in random directions. Rather, at this point, it is the subordinates constraining the leader to lead, creating "vacuum of function" that can and will be inevitably "filled" by any person, however inexperienced relative to the system. However random the commands of the inexperienced leader are, they will be "brought back into range" by the rest of the system, to the nearest command out of the set of commands that the system understands; or to no command, in that the system simply refuses to execute the command (either explicitly or through "presence" in the leader itself of the innate understanding that a car selling company won't place cans of beer on a horse's back; also a constraint imposed on the leader). Indeed, the system may be so "smart" it does not need a leader; but uses the leader as an "option", as voluntary centralization for perhaps speed of decision making and absence of costly consensus. Thus the smallest word, the tiniest nod will work; and the leader need not even know what that "nod" or "word" means.
